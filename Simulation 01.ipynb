{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7e0b1d",
   "metadata": {},
   "source": [
    "## Importing the relevant library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33cb63ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e4de78",
   "metadata": {},
   "source": [
    "For this simulation, we will try using both Pytorch and Keras to see how they stack against each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402c424",
   "metadata": {},
   "source": [
    "## Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a59e192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations applied on the dataset (converting to tensor and normalizing)\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Downloading the MNIST dataset for training and testing\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                       download=True, transform=transform)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                      download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100,\n",
    "                                           shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100,\n",
    "                                          shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0076268",
   "metadata": {},
   "source": [
    "## Printing a sample of 5 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe338fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Print a sample of 5 images\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize the images\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Getting some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Showing 5 images\n",
    "imshow(torchvision.utils.make_grid(images[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168da36",
   "metadata": {},
   "source": [
    "# # Defining the CNN model based on the paper using PyTorch Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20a70989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Flatten(start_dim=1, end_dim=-1)\n",
      "  (7): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (8): ReLU()\n",
      "  (9): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# CNN model based on \"Communication-Efficient Learning of Deep Networks from Decentralized Data\"\n",
    "\n",
    "cnn = nn.Sequential(\n",
    "    # Layer 1: Convolutional layer with 32 filters, 5x5 kernel, stride of 1, and ReLU activation\n",
    "    nn.Conv2d(1, 32, 5, stride=1),\n",
    "    nn.ReLU(),\n",
    "    # Layer 2: Max pooling with 2x2 kernel and stride of 2\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    # Layer 3: Convolutional layer with 64 filters, 5x5 kernel, stride of 1, and ReLU activation\n",
    "    nn.Conv2d(32, 64, 5, stride=1),\n",
    "    nn.ReLU(),\n",
    "    # Layer 4: Max pooling with 2x2 kernel and stride of 2\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    # Flattening the tensor before feeding it into the fully connected layer\n",
    "    nn.Flatten(),\n",
    "    # Layer 5: Fully connected layer with 512 units and ReLU activation\n",
    "    nn.Linear(64 * 4 * 4, 512),\n",
    "    nn.ReLU(),\n",
    "    # Layer 6: Fully connected layer with 10 units (for 10 output classes)\n",
    "    nn.Linear(512, 10)\n",
    ")\n",
    "\n",
    "print(cnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31a9d594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.08598741213262352\n",
      "Epoch 2, Loss: 0.041083254471886904\n",
      "Epoch 3, Loss: 0.03444581552981011\n",
      "Epoch 4, Loss: 0.031602944134404724\n",
      "Epoch 5, Loss: 0.027049394093143442\n",
      "Epoch 6, Loss: 0.028618493826070335\n",
      "Epoch 7, Loss: 0.026918025603954447\n",
      "Epoch 8, Loss: 0.02674050654876434\n",
      "Epoch 9, Loss: 0.026569154609169345\n",
      "Epoch 10, Loss: 0.025320614635274977\n",
      "Epoch 11, Loss: 0.023841124325602626\n",
      "Epoch 12, Loss: 0.024820176782717075\n",
      "Epoch 13, Loss: 0.024314920421456918\n",
      "Epoch 14, Loss: 0.025116471052266814\n",
      "Epoch 15, Loss: 0.022294397143802294\n",
      "Epoch 16, Loss: 0.024505533153618066\n",
      "Epoch 17, Loss: 0.022009988683469903\n",
      "Epoch 18, Loss: 0.021782199515170457\n",
      "Epoch 19, Loss: 0.025752266226530384\n",
      "Epoch 20, Loss: 0.02315253838072143\n",
      "Epoch 21, Loss: 0.02304718744494797\n",
      "Epoch 22, Loss: 0.02121050353787723\n",
      "Epoch 23, Loss: 0.021856384824447257\n",
      "Epoch 24, Loss: 0.022824267214309656\n",
      "Epoch 25, Loss: 0.022225706858589548\n",
      "Epoch 26, Loss: 0.023140228576982433\n",
      "Epoch 27, Loss: 0.021662739373908457\n",
      "Epoch 28, Loss: 0.020703972473274917\n",
      "Epoch 29, Loss: 0.02162452056991848\n",
      "Epoch 30, Loss: 0.021484729762014466\n",
      "Epoch 31, Loss: 0.022940547857627582\n",
      "Epoch 32, Loss: 0.021741779823569233\n",
      "Epoch 33, Loss: 0.02392165111406939\n",
      "Epoch 34, Loss: 0.021863158049042493\n",
      "Epoch 35, Loss: 0.021528471822627276\n",
      "Epoch 36, Loss: 0.02401005286902849\n",
      "Epoch 37, Loss: 0.020709209016980215\n",
      "Epoch 38, Loss: 0.021489710864746787\n",
      "Epoch 39, Loss: 0.0201495448598871\n",
      "Epoch 40, Loss: 0.02265976543094439\n",
      "Epoch 41, Loss: 0.02008905604869748\n",
      "Epoch 42, Loss: 0.02327622435836626\n",
      "Epoch 43, Loss: 0.023160223983456185\n",
      "Epoch 44, Loss: 0.020259688007596803\n",
      "Epoch 45, Loss: 0.023615165785401283\n",
      "Epoch 46, Loss: 0.020113938811506766\n",
      "Epoch 47, Loss: 0.019785706682159798\n",
      "Epoch 48, Loss: 0.019881817516240213\n",
      "Epoch 49, Loss: 0.02022434372125038\n",
      "Epoch 50, Loss: 0.0225695099641113\n",
      "Epoch 51, Loss: 0.007651684937057628\n",
      "Epoch 52, Loss: 0.004454156819435108\n",
      "Epoch 53, Loss: 0.0036833439705029982\n",
      "Epoch 54, Loss: 0.0033427408262286917\n",
      "Epoch 55, Loss: 0.0031697872617223767\n",
      "Epoch 56, Loss: 0.0029812122054499925\n",
      "Epoch 57, Loss: 0.0029906279076506812\n",
      "Epoch 58, Loss: 0.00299011519888154\n",
      "Epoch 59, Loss: 0.003007379113648009\n",
      "Epoch 60, Loss: 0.0029630294282954614\n",
      "Epoch 61, Loss: 0.0030232221027836205\n",
      "Epoch 62, Loss: 0.0031008225363621023\n",
      "Epoch 63, Loss: 0.003146916515591632\n",
      "Epoch 64, Loss: 0.0031898597556331274\n",
      "Epoch 65, Loss: 0.003193160167744888\n",
      "Epoch 66, Loss: 0.0032697904345210796\n",
      "Epoch 67, Loss: 0.003297669089927998\n",
      "Epoch 68, Loss: 0.0032960228978239077\n",
      "Epoch 69, Loss: 0.003403506562851059\n",
      "Epoch 70, Loss: 0.0034048693152726626\n",
      "Epoch 71, Loss: 0.0033571021934524956\n",
      "Epoch 72, Loss: 0.003369176172297254\n",
      "Epoch 73, Loss: 0.0033785297773526205\n",
      "Epoch 74, Loss: 0.003464317616938691\n",
      "Epoch 75, Loss: 0.003358517304562459\n",
      "Epoch 76, Loss: 0.0034513819052275113\n",
      "Epoch 77, Loss: 0.003434432845048529\n",
      "Epoch 78, Loss: 0.003427005984922289\n",
      "Epoch 79, Loss: 0.0033382094928674634\n",
      "Epoch 80, Loss: 0.003402237941069567\n",
      "Epoch 81, Loss: 0.0034526870422026453\n",
      "Epoch 82, Loss: 0.0034188481504800923\n",
      "Epoch 83, Loss: 0.003436098843206613\n",
      "Epoch 84, Loss: 0.003429882408963749\n",
      "Epoch 85, Loss: 0.0034749884061845175\n",
      "Epoch 86, Loss: 0.003456189269248474\n",
      "Epoch 87, Loss: 0.003450874852205743\n",
      "Epoch 88, Loss: 0.0033710053074901226\n",
      "Epoch 89, Loss: 0.0033752834942909734\n",
      "Epoch 90, Loss: 0.0033040035989567213\n",
      "Epoch 91, Loss: 0.003426996771231643\n",
      "Epoch 92, Loss: 0.003420193933270639\n",
      "Epoch 93, Loss: 0.0033515731026030455\n",
      "Epoch 94, Loss: 0.003413421029011564\n",
      "Epoch 95, Loss: 0.0033849917334737257\n",
      "Epoch 96, Loss: 0.003359334936612868\n",
      "Epoch 97, Loss: 0.0033635611346107906\n",
      "Epoch 98, Loss: 0.0034239617600057195\n",
      "Epoch 99, Loss: 0.003451841016067192\n",
      "Epoch 100, Loss: 0.00328605293098614\n",
      "Epoch 101, Loss: 0.002708496273990022\n",
      "Epoch 102, Loss: 0.002566459663551844\n",
      "Epoch 103, Loss: 0.002539929843284578\n",
      "Epoch 104, Loss: 0.002534451841468884\n",
      "Epoch 105, Loss: 0.002528222617353701\n",
      "Epoch 106, Loss: 0.002528219650121173\n",
      "Epoch 107, Loss: 0.002531988088594517\n",
      "Epoch 108, Loss: 0.0025413357681342555\n",
      "Epoch 109, Loss: 0.0025465372617084845\n",
      "Epoch 110, Loss: 0.0025502354982018006\n",
      "Epoch 111, Loss: 0.0025602861197451905\n",
      "Epoch 112, Loss: 0.002558023430465255\n",
      "Epoch 113, Loss: 0.0025797717272265193\n",
      "Epoch 114, Loss: 0.002573796337407354\n",
      "Epoch 115, Loss: 0.0025778698121818404\n",
      "Epoch 116, Loss: 0.002582214524857894\n",
      "Epoch 117, Loss: 0.0025984145841115\n",
      "Epoch 118, Loss: 0.0026023696828027217\n",
      "Epoch 119, Loss: 0.0026013365131802857\n",
      "Epoch 120, Loss: 0.0026139206757216015\n",
      "Epoch 121, Loss: 0.002616409249470356\n",
      "Epoch 122, Loss: 0.0026192428199768377\n",
      "Epoch 123, Loss: 0.0026250465658085886\n",
      "Epoch 124, Loss: 0.0026247378463449423\n",
      "Epoch 125, Loss: 0.0026323062939142498\n",
      "Epoch 126, Loss: 0.0026412790239070697\n",
      "Epoch 127, Loss: 0.002637851052180243\n",
      "Epoch 128, Loss: 0.002647275483177509\n",
      "Epoch 129, Loss: 0.002651323857486811\n",
      "Epoch 130, Loss: 0.0026561700675665633\n",
      "Epoch 131, Loss: 0.002666272621039146\n",
      "Epoch 132, Loss: 0.0026598373254576777\n",
      "Epoch 133, Loss: 0.0026716340304847107\n",
      "Epoch 134, Loss: 0.0026676769546854\n",
      "Epoch 135, Loss: 0.002677466832465143\n",
      "Epoch 136, Loss: 0.002681616737681907\n",
      "Epoch 137, Loss: 0.0026859687612886774\n",
      "Epoch 138, Loss: 0.002687901639267996\n",
      "Epoch 139, Loss: 0.0026842510695860257\n",
      "Epoch 140, Loss: 0.0026895958046467662\n",
      "Epoch 141, Loss: 0.002696438869315898\n",
      "Epoch 142, Loss: 0.002691299091966357\n",
      "Epoch 143, Loss: 0.0027061080706577436\n",
      "Epoch 144, Loss: 0.0027019983928281968\n",
      "Epoch 145, Loss: 0.0027055856138031233\n",
      "Epoch 146, Loss: 0.002708792466049393\n",
      "Epoch 147, Loss: 0.002709717861022606\n",
      "Epoch 148, Loss: 0.002703909659491425\n",
      "Epoch 149, Loss: 0.002716963350394508\n",
      "Epoch 150, Loss: 0.002709831296694271\n",
      "Epoch 151, Loss: 0.002670359280818957\n",
      "Epoch 152, Loss: 0.0026538326950443056\n",
      "Epoch 153, Loss: 0.0026503561411664123\n",
      "Epoch 154, Loss: 0.002648839823329278\n",
      "Epoch 155, Loss: 0.002648445205510749\n",
      "Epoch 156, Loss: 0.0026479938367022743\n",
      "Epoch 157, Loss: 0.002647922277925924\n",
      "Epoch 158, Loss: 0.002648354008803532\n",
      "Epoch 159, Loss: 0.002648980799034083\n",
      "Epoch 160, Loss: 0.002648534820060983\n",
      "Epoch 161, Loss: 0.0026496269519460233\n",
      "Epoch 162, Loss: 0.0026488981537598497\n",
      "Epoch 163, Loss: 0.0026493558821675833\n",
      "Epoch 164, Loss: 0.0026491539426933743\n",
      "Epoch 165, Loss: 0.0026492778488318436\n",
      "Epoch 166, Loss: 0.0026496402111661154\n",
      "Epoch 167, Loss: 0.002649271027718593\n",
      "Epoch 168, Loss: 0.0026499744395550805\n",
      "Epoch 169, Loss: 0.0026506495186670994\n",
      "Epoch 170, Loss: 0.0026511011504044293\n",
      "Epoch 171, Loss: 0.0026509982676119156\n",
      "Epoch 172, Loss: 0.002650805187925774\n",
      "Epoch 173, Loss: 0.002651966997461083\n",
      "Epoch 174, Loss: 0.0026510123002905553\n",
      "Epoch 175, Loss: 0.0026517881674711437\n",
      "Epoch 176, Loss: 0.0026528160194963374\n",
      "Epoch 177, Loss: 0.002652253942699948\n",
      "Epoch 178, Loss: 0.0026529026207572316\n",
      "Epoch 179, Loss: 0.002653048728243448\n",
      "Epoch 180, Loss: 0.0026528548439091536\n",
      "Epoch 181, Loss: 0.0026536440386553295\n",
      "Epoch 182, Loss: 0.002653586544911377\n",
      "Epoch 183, Loss: 0.0026534626576297645\n",
      "Epoch 184, Loss: 0.00265400350993635\n",
      "Epoch 185, Loss: 0.002654671962421465\n",
      "Epoch 186, Loss: 0.002654718142985075\n",
      "Epoch 187, Loss: 0.0026546335361733024\n",
      "Epoch 188, Loss: 0.002655088765726153\n",
      "Epoch 189, Loss: 0.0026552860332109654\n",
      "Epoch 190, Loss: 0.0026556735860018914\n",
      "Epoch 191, Loss: 0.0026555667196953435\n",
      "Epoch 192, Loss: 0.0026562264775081227\n",
      "Epoch 193, Loss: 0.0026560243821829015\n",
      "Epoch 194, Loss: 0.002656458630178046\n",
      "Epoch 195, Loss: 0.0026567513900226914\n",
      "Epoch 196, Loss: 0.0026566738024606214\n",
      "Epoch 197, Loss: 0.0026574593167363976\n",
      "Epoch 198, Loss: 0.0026577936686226168\n",
      "Epoch 199, Loss: 0.0026579162904818076\n",
      "Epoch 200, Loss: 0.002657878044847166\n",
      "Finished training the CNN model\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Move the model to the device\n",
    "cnn = cnn.to(device)\n",
    "\n",
    "# Set the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Adjust learning rate during training\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial learning rate decayed by 10 every 50 epochs\"\"\"\n",
    "    lr = 0.1 * (0.1 ** (epoch // 50))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 200\n",
    "\n",
    "# Train the CNN model\n",
    "for epoch in range(num_epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = cnn(inputs)\n",
    "        # Calculate loss\n",
    "        loss = loss_function(outputs, labels)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / (i + 1)}\")\n",
    "\n",
    "print(\"Finished training the CNN model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "248d8536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the CNN model on the test dataset: 99.44%\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "cnn.eval()\n",
    "\n",
    "# Initialize variables to count correct predictions and total predictions\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Disable gradient calculations during evaluation (saves memory and speeds up computation)\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = cnn(inputs)\n",
    "\n",
    "        # Get the predicted class by selecting the class with the highest score\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Update the total number of predictions\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Update the number of correct predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Accuracy of the CNN model on the test dataset: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfadac90",
   "metadata": {},
   "source": [
    "# Using the Keras Module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27fe2225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 18s 9ms/step - loss: 0.4483 - accuracy: 0.8735\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.1133 - accuracy: 0.9661\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0789 - accuracy: 0.9764\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0625 - accuracy: 0.9812\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 18s 9ms/step - loss: 0.0525 - accuracy: 0.9837\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 18s 9ms/step - loss: 0.0454 - accuracy: 0.9860\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 18s 9ms/step - loss: 0.0404 - accuracy: 0.9876\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0348 - accuracy: 0.9894\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0320 - accuracy: 0.9900\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 18s 9ms/step - loss: 0.0291 - accuracy: 0.9912\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0328 - accuracy: 0.9881\n",
      "Accuracy of the CNN model on the test dataset: 98.81%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "x_test = x_test.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Define the CNN model\n",
    "cnn = models.Sequential([\n",
    "    layers.Conv2D(32, (5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (5, 5), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "cnn.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN model\n",
    "num_epochs = 10\n",
    "cnn.fit(x_train, y_train, epochs=num_epochs, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = cnn.evaluate(x_test, y_test)\n",
    "print(f\"Accuracy of the CNN model on the test dataset: {test_accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Simulation01",
   "language": "python",
   "name": "simulation01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
